\input{settings.tex}
\linespread{1.6} % Space between lines
\parindent=0pt % line up
\begin{document}
	\section*{Chapter 2 Linear Transformations and Matrices}	
	\subsection*{\S\,2-1 Linear transformation, Null Spaces, and Ranges}
\begin{defn}[Definition of Linear Transformation]
$ $\\	Let $\mathrm{V}$ and $\mathrm{W}$ be vector spaces (over $\mathrm{F}$). We call a function $\mathrm{T}: \mathrm{V}  \rightarrow  \mathrm{W}$    a linear transformation from $\mathrm{V}$ to $\mathrm{W}$ if, for all x, y $\in$ $\mathrm{V}$ and c $\in$ $\mathrm{F}$, we have

\begin{enumerate}
	\item [(a)]   $\mathrm{T}$(x + y) = $\mathrm{T}$(x) + $\mathrm{T}$(y) 
	\item [(b)]    $\mathrm{T}$($\mathrm{c}$x) = $\mathrm{c}\mathrm{T}$(x)
\end{enumerate} 
\end{defn}

\begin{defn}[Definition of nullity and rank]
$ $\\	Let $\mathrm{V}$ and $\mathrm{W}$ be vector spaces and let $\mathrm{T}: \mathrm{V}  \rightarrow  \mathrm{W}$ be linear. If N(T) and R(T) are finite-dimensional, then we define the nullity of T, denoted nullity(T), and the rank of T, denoted rank(T), to be the dimensions of N(T) and R(T), respectively.
\end{defn}

\begin{thm*}
$ $ \\ 	Let V and W be vector spaces and $\mathrm{T}: \mathrm{V}  \rightarrow  \mathrm{W}$ be linear. Then N(T) and R(T) are subspaces of $\mathrm{V}$ and $\mathrm{W}$, respectively.
\end{thm*}

\begin{thm*}
$ $\\
 Let V and W be vector spaces, and let $\mathrm{T} : \mathrm{V}  \rightarrow \mathrm{W}$ be linear. If $\beta = \{ v_1,v_2,\cdots,v_n \}$ is a basis for $\mathrm{V}$ , then $ \mathrm{R}(\mathrm{T}) = \mathrm{span}(\mathrm{T}(\beta)) = \mathrm{span}(\{\mathrm{T}(v_1), \mathrm{T}(v_2), . . . , \mathrm{T}(v_n)\})$.

\end{thm*}

\begin{example}[11] %Example 11
	Question : \\
	Let T : $\mathrm{P}_2(R) \rightarrow  \mathrm{P}_3(R) $ be the linear trans formation defined by 
	
		
	 \[T(f(x)) = 2f'(x) + \int_0^x3 f(t) dt \]
	
		\begin{sol*} 
$ $ \\ 	
Now $\mathrm{R}(\mathrm{T}) = \mathrm{Span}( \{ \mathrm{T}(1), \mathrm{T}(x), \mathrm{T}(x^2 )  ) = \mathrm{Span}( \{ 3x,  2+ \frac{3}{2}  x^2 , 4x + x^3 \} ) $. 

 \noindent Since \{ $3x$ , $ 2 + \frac{3}{2} x^2$, $4x + x^3$ \} is linearly independent, rank($\mathrm{T}$) = 3. Since
2
$\dim(P_3(\R)) = 4$ , T is not onto. From the dimension theorem (Thm 2.3) , nullity(T) +
3 = 3. So nullity(T) = 0, and therefore, N(T) = {0}. We conclude from Theorem 2.4 that T is one-to-one.

	\end{sol*}

	\end{example}
\begin{example}[12] %Example 12
	 Question : \\ Let $\mathrm{T}: \mathrm{F}^2 \rightarrow \mathrm{F}^2 $ be the linear transformation defined by 
	 
	 \[T(a_1, a_2) = (a_1 + a_2, a_1)\]

	\begin{sol*} 
$ $ \\
		It is easy to see that N(T) = \{ 0 \} ; so T is one-to-one. Hence Theorem 2.5 tells us that T must be onto. 
	\end{sol*}

\end{example}

\begin{thm*}[2.3 Dimension Theorem]
$ $ \\	
Let V and W be vector spaces,
and let $ \mathrm{T} : \mathrm{V}  \rightarrow  \mathrm{W} $ be linear. \\ If V is finite-dimensional, then
 
 
 \[\rm	nullity(T) + rank(T) = dim(V).\]


\end{thm*}

\begin{thm*}[2.4]
$ $ \\	Let V and W be vector spaces, and let $\rm T: V  \rightarrow  W$ be
linear. Then T is one-to-one if and only if N(T) = {0 }.

\end{thm*}

\begin{thm*}[2.5]
$ $ \\	 Let V and W be vector spaces of equal (finite) dimension, and let $\rm T: V  \rightarrow  W$ be linear. Then the following are equivalent.
\begin{enumerate} 
	\item[(a)] T is one-to-one. 
	\item[(b)] T is onto.
	\item[(c)] rank(T) = dim(V).

\end{enumerate}

\end{thm*}

\begin{example}
	Let $\rm T : \R^2\longrightarrow \R^3 $ , $\rm T(a_1,a_2) = (a_1+a_2 , 0 , 2a_1-a_2)$ , determine that $\rm T $ is linear , 1-1 , onto or not.
	\begin{sol*}
		 $ $
		\begin{itemize}
			\item Calim : $\rm T$ is linear.\newline
			Let $ x =(a_1,a_2) \,,\, y= (b_1,b_2)$ 
				
				\(\begin{aligned} \rm T(c x+y) &=\rm T\left(c\left(a_{1}, a_{2}\right)+\left(b_{1}, b_{2}\right)\right)= T\left(ca_{1}+b_{1}, ca_{2}+b_{2}\right) \\ &=\rm \left(c a_{1}+b_{1}+c a_{2}+b_{2}, 0,2 c a_{1}+2 b_{1}-ca_{2}-b_{2}\right) \\ &=\rm \left(c\left(a_{1}+a_{2}\right)+\left(b_{1}+b_{2}\right), 0, c\left(2 a_{1}-a_{2}\right)+\left(2 b_{1}-b_{2}\right)\right) \\ &=\rm c\left(a_{1}+a_{2}, 0,2 a_{1}-a_{2}\right)+\left(b_{1}+b_{2}, 0,2 b_{1}-b_{2}\right) \\ &=\rm c T(x)+T (y) \end{aligned}\)
				
			$\therefore \rm T $ is linear
			\item 1-1 and onto 	
			
			By Thm 2.2 in text book choose a basis for $\R^2 , \beta = \{(1,0) , (0,1)\}$
			
			$\rm R(T) = span\left(T(\beta)\right) = span(\left\{T(1,0) , T(0,1)\right\}) = span\left((1,0,-1), (1,0,-2)\right)$ 
			
			Clearly it is L.I. $\rm\implies rank(T) = 2$ , and apply the Dimension Theorem $\rm rank(T)=2\neq3=\dim(\R^3) $, and $\rm nullitily(T) = 1 $ , then it is not onto.
			
			By Thm 2.4  it is not one to one.
			
		\end{itemize}
	\end{sol*}
		\end{example}
		\begin{example}
		Let \(\rm V\) and \(\rm W\) be vector spaces, let \(\mathrm{T}:\rm  V \rightarrow W\) be linear, and let
\(\left\{w_{1}, w_{2}, \ldots, w_{k}\right\}\) be a linearly independent subset of \(\rm R(T)\). Prove that
if \(S=\left\{v_{1}, v_{2}, \ldots, v_{k}\right\}\) is chosen so that \(\mathrm{T}\left(v_{i}\right)=w_{i}\) for \(i=1,2, \ldots, k\),
then \(S\) is linearly independent.
\begin{sol*}$ $
$ $\\
Calim : $\sum_{i=1}^n a_iv_i = 0\implies a_1 = a_2 = a_3 =\cdots =a_n = 0 $ 


Let \(\sum_{i=1}^{n} a_{i} v_{i}=0\)
then \(\mathrm{T}\left(\sum_{i=1}^{n} a_{i} v_{i}\right)=0\)
$ $\\
Since \(\rm T\) is linear , \(\mathrm{T}\left(\sum_{i=1}^{n} a_{i} v_{i}\right)=\sum_{i=1}^{n} a_{i} \mathrm{T}\left(v_{i}  \right)=\sum_{i=1}^na_iw_i = 0\)


Since $\mathrm{S}$ is L.I. $\implies a_1 = a_2=\cdots = a_n = 0$ 
\end{sol*}
	

\end{example}
\begin{thm*}[2.6]
$ $ \\Let V and W be vector spaces over F, and suppose that \{ $v_1,v_2,\cdots,v_n$ \} is a basis for V. For $w_1,w_2,\cdots,w_n$ in W, there exists exactly one linear transformation $\mathrm{T}: \mathrm{V}  \rightarrow \mathrm{W}$ such that T($v_i$) = $w_i$ for i = 1,2,$\cdots$,n.
\end{thm*}
		
\subsection*{\S\,2-2 The Matrix Representation of a Linear transformation}

\begin{defn}[Definition of an Ordered Basis]
$ $ \\  Let V be a finite-dimensional vector space. An ordered basis for V is a basis for V endowed with a specific order; that is, an ordered basis for V is a finite sequence of linearly independent vectors in V that generates V.
\end{defn}

\newpage
\begin{defn}[Definition for the symbol $\lbrack x \rbrack_\beta$]
$ $\\ Let $\beta = \{ u_1,u_2,\cdots,u_n \}$ be an ordered basis for a finite- dimensional vector space V. For $x \in \mathrm{V}$, let $a_1, a_2, \cdot $, an be the unique scalars such that
$x = \sum_{i=1}^{\mathrm{n}} a_iu_i$
We define the coordinate vector of x relative to $\beta$, denoted $\lbrack x \rbrack_\beta$, by
\begin{center}
	 $\lbrack x \rbrack_\beta = \left[\begin{matrix}
	a_1 \\
	a_2 \\
	\vdots \\
	a_n 	
\end{matrix}\right]$
\end{center}

Notice that $\lbrack u_i \rbrack_\beta = e_i$ in the preceding definition. It is left as an exercise to show that the correspondence $x \rightarrow \lbrack x \rbrack_ \beta$ provides us with a linear transformation from V to $\mathrm{F}^n$. We study this transformation in Section 2.4 in more detail.
\end{defn}

% Textbook 2-2 Exercise 8

\begin{defn}[Definition for the symbol $\lbrack \mathrm{T} \rbrack^\gamma_\beta$] $ $\\
Using the notation above,we call the $\mathrm{m} \times \mathrm{n}$ matrix A defined by $\mathrm{A}_{ij} = \mathrm{a}_{ij}$ the matrix representation of T in the ordered bases $\beta$ and $\gamma$ and write $\mathrm{A} = \lbrack \mathrm{T} \rbrack^\gamma_\beta$. If $\mathrm{V} = \mathrm{W}$ and $\beta = \gamma$, then we write $\mathrm{A} = \lbrack \mathrm{T}\rbrack_\beta$.
Notice that the jth column of A is simply $\lbrack \mathrm{T} (v_j)\rbrack_\gamma$ . Also observe that if
$\mathrm{U} : \mathrm{V} \rightarrow \mathrm{W}$ is a linear transformation such that $\lbrack \mathrm{U} \rbrack^\gamma_\beta = [\mathrm{T}]^\gamma_\beta$, then $\mathrm{U} = \mathrm{T}$ by the corollary to Theorem 2.6 (p. 73).

\end{defn}

\begin{defn}[The addition and multiplication for Linear Transformation] $ $\\
Let $\mathrm{T} , \mathrm{U}: \mathrm{V} \rightarrow \mathrm{W}$ be arbitrary functions, where V and W are vector spaces over F, and let $a \in \mathrm{F}$. We define $\mathrm{T}+\mathrm{U} : \mathrm{V} \rightarrow \mathrm{W}$ \ \ by\ \ $(\mathrm{T}+\mathrm{U})(x) = \mathrm{T}(x)+\mathrm{U}(x)$\ \ for all $x \in \mathrm{V}$, and $a\mathrm{T}: \mathrm{V} \rightarrow \mathrm{W}$\ \ by\ \ $(a\mathrm{T})(x) = a\mathrm{T}(x)$\ \ for all \ \ $x \in \mathrm{V}$.


Of course, these are just the usual definitions of addition and scalar multiplication of functions. We are fortunate, however, to have the result that both sums and scalar multiples of linear transformations are also linear.
\end{defn}

\newpage

\begin{thm*}[2.7] $ $\\
	 Let V and W be vector spaces over a field $F$, and let\ \ $\mathrm{T} , \mathrm{U}: \mathrm{V} \rightarrow \mathrm{W}$\ \ be linear.
\begin{enumerate} 
	\item[(a)] For all $a \in F$,\ \ $a$$\mathrm{T} + \mathrm{U}$ is linear.
	\item[(b)] Using the operations of addition and scalar multiplication in the preceding definition, the collection of all linear transformations from V to W is a vector space over $F$.
\end{enumerate}
	
\end{thm*}

\begin{defn}[Definition of the Linear Operator] $ $\\
Let V and W be vector spaces over $F$. We denote the vector space of all linear transformations from V into W by L(V, W). In the case that V = W, we write L(V) instead of L(V, W).
\end{defn}


\begin{thm*}[2.8]$ $\\
	Let V and W be finite-dimensional vector spaces with ordered bases $\beta$ and $\gamma$, respectively, and let $\mathrm{T} , \mathrm{U}: \mathrm{V} \rightarrow \mathrm{W}$ be linear transformations. Then
\begin{enumerate}
	\item[(a)] $[\mathrm{T} + \mathrm{U}]^\gamma_\beta = [\mathrm{T}]^\gamma_\beta + [\mathrm{U}]^\gamma_\beta$ and
	\item[(b)] $[a\mathrm{T}]^\gamma_\beta = a[\mathrm{T}]^\gamma_\beta$ for all scalars $a$.
\end{enumerate}
$\mathcal{Q} snake$ : Thm 2.8 means that $\lbrack \mathrm{x} \rbrack^\gamma_\beta$ has the characteristic of " linear ", too.
\end{thm*}

\newpage

\color{red} P.S. We suggests you study 2-4 first, and study 2-3 later 

\subsection*{\color{black}\S\,2-3 Composition of Linear Transform}

\color{black}

\begin{thm*}[2.9]$ $\\
	Let V, W, and Z be vector spaces over the same field $F$, and let $\mathrm{T} : \mathrm{V} \rightarrow \mathrm{W}$ and $\mathrm{U} : \mathrm{W} \rightarrow \mathrm{Z}$ be linear. Then $\mathrm{U}\mathrm{T} : \mathrm{V} \rightarrow \mathrm{Z} $ is linear.
\end{thm*}


\begin{thm*}[2.10]$ $\\ 
	Let V be a vector space. Let T, $\mathrm{U}_1$, $\mathrm{U}_2 \in L(V)$. Then 
	\begin{enumerate}
		\item [(a)] $\mathrm{T}(\mathrm{U}_1+\mathrm{U}_2) = \mathrm{T}\mathrm{U}_1 + \mathrm{T}\mathrm{U}_2$ and $(\mathrm{U}_1 + \mathrm{U}_2)\mathrm{T}$ = $\mathrm{U}_1\mathrm{T} + \mathrm{U}_2\mathrm{T}$
		\item [(b)] $\mathrm{T}(\mathrm{U}_1\mathrm{U}_2) = (\mathrm{T}\mathrm{U}_1)\mathrm{U}_2$
		\item [(c)] TI = IT = T
		\item [(d)] $a(\mathrm{U}_1\mathrm{U}_2) = (a\mathrm{U}_1)\mathrm{U}_2 = \mathrm{U}_1(a\mathrm{U}_2)$ for all scalars $a$.
	\end{enumerate}

\end{thm*}

\begin{defn}[Definition for the product of two matrices] $ $\\
	Let A be an m $\times$ n matrix and B be an n $\times$ p matrix. We define the product of A and B, denoted AB, to be the m $\times$ p matrix such that 
	\begin{center}
		$(AB)_{ij}  = \sum_{k=1}^{\mathrm{n}} A_{ik}B_{kj}$ \ \ for $1 \leq i \leq m,$\ \ $1 \leq j \leq p$ 
	\end{center}
\end{defn}

\begin{thm*}[2.11]$ $\\
	Let V, W, and Z be finite-dimensional vector spaces with ordered bases $\alpha , \beta$, and  $\gamma$, respectively. Let $\mathrm{T}: \mathrm{V} \rightarrow \mathrm{w}$ and $\mathrm{U}: \mathrm{W} \rightarrow \mathrm{Z}$ be linear transformations. Then
	\begin{center}
		$[\mathrm{U}\mathrm{T}]^\gamma_\alpha = [\mathrm{U}]^\gamma_\beta[\mathrm{T}]^\beta_\alpha$.
	\end{center}

\end{thm*}
Corollary. $ $\\
Let V be a finite-dimensional vector space with an ordered
basis $\beta$. Let $\mathrm{T}, \mathrm{U} \in L(V)$. Then $[\mathrm{U}\mathrm{T}]_\beta = [\mathrm{U}]_\beta [\mathrm{T}]_\beta$.
$ $\\

\newpage

\begin{defn}[Definition of the Identity Matrix] $ $\\
We define the Kronecker delta $\delta_{ij}$ by $\delta_{ij} = 1$ if $i = j$ and $\delta_{ij} = 0$ if $i \neq j$. The n $\times$ n identity matrix In is defined by $(I_n)_{ij} = \delta_{ij}$.
Thus, for example,

\begin{center}

$I_1 = \left[\begin{matrix}
	1	
\end{matrix}\right]$,\ \ 
$I_2 = \left[\begin{matrix}
	1 & 0\\  
	0 & 1 \\	
\end{matrix}\right]$,\ \ and   
$I_3 = \left[\begin{matrix}
	1 & 0 & 0 \\
	0 & 1 & 0 \\
	0 & 0 & 1 \\ 	
\end{matrix}\right]$

\end{center}
\end{defn}

\begin{thm*}[2.12] $ $\\
 Let A be an m $\times$ n matrix, B and C be n $\times$ p matrices, and D and E be q $\times$ m matrices. Then
 
 \begin{enumerate}
 
 	\item [(a)]A(B+C) = AB + AC and (D+E)A = DA + EA.
 	\item [(b)]$a$(AB) = ($a$A)B = A($a$B) for any scalar $a$.
 	\item [(c)]$\mathrm{I}_mA = A = A\mathrm{I}_n$.
 	\item [(d)]If V is an n-dimensional vector space with an ordered basis $\beta$, then $[I_V]_\beta = I_n$.

 \end{enumerate}

\end{thm*}

\begin{thm*}[2.13] $ $\\
Let A be an m $\times$ n matrix and B be an n $\times$ p matrix. For each $ j (1 \leq j \leq p)$ let $u_j$ and $v_j$ denote the jth columns of AB and B, respectively. Then
\begin{enumerate}
	\item [(a)] $u_j = Av_j$
	\item [(b)] $v_j = Be_j$, where $e_j$ is the jth standard vector of $\mathrm{F}^p$.
 \end{enumerate}	
\end{thm*}

\begin{thm*}[2.14] $ $\\
	Let V and W be finite-dimensional vector spaces having ordered bases $\beta$ and $\gamma$, respectively, and let $\mathrm{T} : \mathrm{V} \rightarrow \mathrm{W}$ be linear. Then, for each $u \in \mathrm{V}$, we have
	\begin{center}
		$[\mathrm{T}(u)]_\gamma = [\mathrm{T}]^\gamma_\beta[u]_\beta$.
	\end{center}

\end{thm*}

\begin{defn}[Definition of Left-Multiplication Transformation]$ $\\
Let A be an m $\times$ n matrix with entries from a field F . We denote by $L_A$ the mapping $L_A : \mathrm{F}^n \rightarrow \mathrm{F}^m$ defined by $L_A(x) = Ax$ (the matrix product of $A$ and $x$) for each column vector $x \in \mathrm{F}^n$. We call $L_A$ a left-multiplication transformation.	
\end{defn}

\newpage

\begin{thm*}[2.15]
	The characteristics of Left-Multiplication Transformation $ $\\
	Let A be an m $\times$ n matrix with entries from $F$ . Then the left-multiplication transformation $\mathrm{L}_A : \mathrm{F}^n \rightarrow \mathrm{F}^m$ is linear. Furthermore, if B is any other m $\times$ n matrix (with entries from F) and $\beta$ and $\gamma$ are the standard ordered bases for $\mathrm{F}^n$ and $\mathrm{F}^m$, respectively, then we have the following properties.
	
	\begin{enumerate}
	
		\item [(a)] $[\mathrm{L}_A]^\gamma_\beta = A$.
		\item [(b)] $\mathrm{L}_A = \mathrm{L}_B$ if and only if A = B.
		\item [(c)] $\mathrm{L}_{A+B} = \mathrm{L}_A + \mathrm{L}_B$ and $\mathrm{L}_{aA} = a\mathrm{L}_A$ for all $a \in F$.
		\item [(d)]  If $\mathrm{T} : F^n \rightarrow F^m$ is linear, then there exists a unique m $\times$ n matrix C such
that $\mathrm{T} = \mathrm{L}_C$. In fact, $C = [\mathrm{T}]^\gamma_\beta$.
		\item [(e)] If E is an n $\times$ p matrix,then $\mathrm{L}_{AE} = \mathrm{L}_A\mathrm{L}_E$.
		\item [(f)] If m = n L, then $\mathrm{L}_{I_n} =\mathrm{I}_{F^n}$.
	
	\end{enumerate}

\end{thm*}

\begin{thm*}[2.16]$ $\\
Let A,B, and C be matrices such that A(BC) is defined. Then (AB)C is also defined and A(BC) = (AB)C ; that is, matrix multiplication is associative.	
\end{thm*}

\newpage

\begin{defn}[Definition of Innvertible] $ $\\
	Let V and W be vector spaces, and let $\mathrm{T} : \mathrm{V} \rightarrow \mathrm{W}$ be linear. A function $\mathrm{U} : \mathrm{W} \rightarrow \mathrm{V}$ is said to be an inverse of T if $\mathrm{T} \mathrm{U} = \mathrm{I}_\mathrm{W}$ and $\mathrm{U} \mathrm{T} = \mathrm{I}_\mathrm{V}$. If T has an inverse, then T is said to be invertible. As noted in Appendix B, if T is invertible, then the inverse of T is unique and is denoted by $\mathrm{T}^{-1}$.

	
\begin{center}
	The following facts hold for invertible functions T and U.
\end{center}
	\begin{enumerate}
	
	\item [(1.)] $(\mathrm{T}\mathrm{U})^{-1} = \mathrm{U}^{-1}\mathrm{T}^{-1}$ is invertible.
	
	\item[(2.)] $(\mathrm{T}^{-1})^{-1} = \mathrm{T}$ ; in particular, $\mathrm{T}^{-1}$ is invertible 

	\item[] We often use the fact that a function is invertible if and only if it is both one-to-one and onto. We can therefore restate Theorem 2.5 as follows.
		
	\item [(3.)] Let $\mathrm{T} : \mathrm{V} \rightarrow \mathrm{W}$ be a linear transformation, where V and W are finite-dimensional spaces of equal dimension. Then T is invertible if and only if rank(T) = $\dim(V)$.
	\end{enumerate}

\end{defn}

\begin{thm*}[2.17]$ $\\
	Let V and W be vector spaces, and let $\mathrm{T} : \mathrm{V} \rightarrow \mathrm{W}$ be linear and invertible. Then $\mathrm{T}^{-1} : \mathrm{V} \rightarrow \mathrm{W}$ is linear.
\end{thm*}

\begin{defn}[Definition of Matrix Invertible]$ $\\
	Let A be an n $\times$ n matrix. Then A is invertible if there exists an n $\times$ n matrix B such that AB = BA = I.
	
	
  If A is invertible, then the matrix B such that AB = BA = I is unique. (If C were another such matrix, then C = CI = C(AB) = (CA)B = IB = B.) The matrix B is called the inverse of A and is denoted by $\mathrm{A}^{-1}$.

\end{defn}



Lemma. 

 Let T be an invertible linear transformation from V to W. Then V is finite-dimensional if and only if W is finite-dimensional. In this case, $\dim(\mathrm{V}) = \dim(\mathrm{W})$.


\newpage

\begin{thm*}[2.18]$ $\\
	Let V and W be finite-dimensional vector spaces with ordered bases $\beta$ and $\gamma$, respectively. Let $\mathrm{T} : \mathrm{V} \rightarrow \mathrm{W}$ be linear. Then T is invertible if and only if $[\mathrm{T}]^\gamma_\beta$ is invertible. Furthermore, $[\mathrm{T}^{-1}]^\beta_\gamma = ([\mathrm{T}^\gamma_\beta])^{-1}$.
	
\end{thm*}

Corollary 1. $ $\\
Let V be a finite-dimensional vector space with an ordered basis $\beta$, and let $\mathrm{T} : \mathrm{V} \rightarrow \mathrm{V}$ be linear. Then T is invertible if and only if $[\mathrm{T}]_\beta$ is invertible.

 Furthermore, $[\mathrm{T}^{-1}]_\beta = ([\mathrm{T}]_\beta)^{-1}$


Corollary 2. $ $\\
Let A be an n $\times$ n matrix. Then A is invertible if and only
if $\mathrm{L}_A$ is invertible.

 Furthermore, $(\mathrm{L_A})^{-1} = \mathrm{L}_{A^{-1}}$

\begin{defn}[Definition of Isomorphism]$ $\\
	Let V and W be vector spaces. We say that V is isomorphic to W if there exists a linear transformation $\mathrm{T} : \mathrm{V} \rightarrow \mathrm{W}$ that is invertible. Such a linear transformation is called an isomorphism from V onto W.
\end{defn}

\begin{thm*}[2.19]$ $\\
Let V and W be finite-dimensional vector spaces (over the same field). Then V is isomorphic to W if and only if $\dim(\mathrm{V}) = \dim(\mathrm{W})$.

\end{thm*}

Corollary.$ $\\
Let V be a vector space over $F$. Then V is isomorphic to $\mathrm{F}^n$
if and only if $\dim(\mathrm{V}) = n$.

\begin{thm*}[2.20]$ $\\
Let V and W be finite-dimensional vector spaces over F of dimensions n and m, respectively, and let $\beta$ and $\gamma$ be ordered bases for V and W, respectively. Then the function $\Phi:\mathrm{L}(\mathrm{V},\mathrm{W}) \rightarrow \mathrm{M}_{m \times n}(F)$ , defined by $\Phi(\mathrm{T}) = [\mathrm{T}]^\gamma_\beta$ for $\mathrm{T} \in \mathrm{L}(\mathrm{V},\mathrm{W})$ , is an isomorphism.
\end{thm*}

Corollary.$ $\\
Let V and W be finite-dimensional vector spaces of dimensions n and m, respectively. Then L(V, W) is finite-dimensional of dimension mn.

\begin{defn}[Definition of Standard Representation of V]$ $\\
Let $\beta$ be an ordered basis for an n-dimensional vector space V over the field $F$. The standard representation of V with respect to $\beta$ is the function $\phi_\beta:\mathrm{V} \rightarrow \mathrm{F}^n$ defined by $\phi_\beta(x)=[x]_\beta$ for each $x \in \mathrm{V}$.	
\end{defn}

\begin{thm*}[2.21]$ $\\
	For any finite-dimensional vector space V with ordered basis $\beta$, $\phi_\beta$ is an isomorphism.
\end{thm*}
















		
\end{document}